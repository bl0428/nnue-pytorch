{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5115e9b-e216-4e3b-a725-70eb27582557",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mM\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnnue_dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfeatures\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/gpfs/bl0428/nnue-pytorch/model.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mranger\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "File \u001b[0;32m/scratch/gpfs/bl0428/nnue-pytorch/ranger.py:27\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# changes 8/31/19 - fix references to *self*.N_sma_threshold;\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# changed eps to 1e-5 as better default than 1e-8.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer, required\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If dim is None it will be chosen automatically\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import model as M\n",
    "import nnue_dataset\n",
    "import features\n",
    "import os\n",
    "import sys\n",
    "import ranger\n",
    "import torch\n",
    "from torch import set_num_threads as t_set_num_threads\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import copy\n",
    "from feature_transformer import DoubleFeatureTransformerSlice\n",
    "\n",
    "def make_data_loaders(train_filenames, val_filenames, feature_set, num_workers, batch_size, filtered, random_fen_skipping, wld_filtered, early_fen_skipping, param_index, main_device, epoch_size, val_size):\n",
    "  # Epoch and validation sizes are arbitrary\n",
    "  features_name = feature_set.name\n",
    "  train_infinite = nnue_dataset.SparseBatchDataset(features_name, train_filenames, batch_size, num_workers=num_workers,\n",
    "                                                   filtered=filtered, random_fen_skipping=random_fen_skipping, wld_filtered=wld_filtered, early_fen_skipping=early_fen_skipping, param_index=param_index, device=main_device)\n",
    "  val_infinite = nnue_dataset.SparseBatchDataset(features_name, val_filenames, batch_size, filtered=filtered,\n",
    "                                                   random_fen_skipping=random_fen_skipping, wld_filtered=wld_filtered, early_fen_skipping=early_fen_skipping, param_index=param_index, device=main_device)\n",
    "  # num_workers has to be 0 for sparse, and 1 for dense\n",
    "  # it currently cannot work in parallel mode but it shouldn't need to\n",
    "  train = DataLoader(nnue_dataset.FixedNumBatchesDataset(train_infinite, (epoch_size + batch_size - 1) // batch_size), batch_size=None, batch_sampler=None)\n",
    "  val = DataLoader(nnue_dataset.FixedNumBatchesDataset(val_infinite, (val_size + batch_size - 1) // batch_size), batch_size=None, batch_sampler=None)\n",
    "  return train, val\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def flatten_once(lst):\n",
    "    return sum(lst, [])\n",
    "\n",
    "def main():\n",
    "  parser = argparse.ArgumentParser(description=\"Trains the network.\")\n",
    "  parser.add_argument(\"datasets\", action='append', nargs='+', help=\"Training datasets (.binpack). Interleaved at chunk level if multiple specified. Same data is used for training and validation if not validation data is specified.\")\n",
    "  parser = pl.Trainer.add_argparse_args(parser)\n",
    "  parser.add_argument(\"--validation-data\", type=str, action='append', nargs='+', dest='validation_datasets', help=\"Validation data to use for validation instead of the training data.\")\n",
    "  parser.add_argument(\"--lambda\", default=1.0, type=float, dest='lambda_', help=\"lambda=1.0 = train on evaluations, lambda=0.0 = train on game results, interpolates between (default=1.0).\")\n",
    "  parser.add_argument(\"--start-lambda\", default=None, type=float, dest='start_lambda', help=\"lambda to use at first epoch.\")\n",
    "  parser.add_argument(\"--end-lambda\", default=None, type=float, dest='end_lambda', help=\"lambda to use at last epoch.\")\n",
    "  parser.add_argument(\"--gamma\", default=0.992, type=float, dest='gamma', help=\"Multiplicative factor applied to the learning rate after every epoch.\")\n",
    "  parser.add_argument(\"--lr\", default=8.75e-4, type=float, dest='lr', help=\"Initial learning rate.\")\n",
    "  parser.add_argument(\"--num-workers\", default=1, type=int, dest='num_workers', help=\"Number of worker threads to use for data loading. Currently only works well for binpack.\")\n",
    "  parser.add_argument(\"--batch-size\", default=-1, type=int, dest='batch_size', help=\"Number of positions per batch / per iteration. Default on GPU = 8192 on CPU = 128.\")\n",
    "  parser.add_argument(\"--threads\", default=-1, type=int, dest='threads', help=\"Number of torch threads to use. Default automatic (cores) .\")\n",
    "  parser.add_argument(\"--seed\", default=42, type=int, dest='seed', help=\"torch seed to use.\")\n",
    "  parser.add_argument(\"--smart-fen-skipping\", action='store_true', dest='smart_fen_skipping_deprecated', help=\"If enabled positions that are bad training targets will be skipped during loading. Default: True, kept for backwards compatibility. This option is ignored\")\n",
    "  parser.add_argument(\"--no-smart-fen-skipping\", action='store_true', dest='no_smart_fen_skipping', help=\"If used then no smart fen skipping will be done. By default smart fen skipping is done.\")\n",
    "  parser.add_argument(\"--no-wld-fen-skipping\", action='store_true', dest='no_wld_fen_skipping', help=\"If used then no wld fen skipping will be done. By default wld fen skipping is done.\")\n",
    "  parser.add_argument(\"--random-fen-skipping\", default=3, type=int, dest='random_fen_skipping', help=\"skip fens randomly on average random_fen_skipping before using one.\")\n",
    "  parser.add_argument(\"--resume-from-model\", dest='resume_from_model', help=\"Initializes training using the weights from the given .pt model\")\n",
    "  parser.add_argument(\"--network-save-period\", type=int, default=20, dest='network_save_period', help=\"Number of epochs between network snapshots. None to disable.\")\n",
    "  parser.add_argument(\"--save-last-network\", type=str2bool, default=True, dest='save_last_network', help=\"Whether to always save the last produced network.\")\n",
    "  parser.add_argument(\"--epoch-size\", type=int, default=100000000, dest='epoch_size', help=\"Number of positions per epoch.\")\n",
    "  parser.add_argument(\"--validation-size\", type=int, default=1000000, dest='validation_size', help=\"Number of positions per validation step.\")\n",
    "  parser.add_argument(\"--param-index\", type=int, default=0, dest='param_index', help=\"Indexing for parameter scans.\")\n",
    "  parser.add_argument(\"--early-fen-skipping\", type=int, default=-1, dest='early_fen_skipping', help=\"Skip n plies from the start.\")\n",
    "  features.add_argparse_args(parser)\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  args.datasets=[\"/scratch/gpfs/bl0428/bin/voc/test_voc.binpack\"]  \n",
    "\n",
    "  args.datasets = flatten_once(args.datasets)\n",
    "  if args.validation_datasets:\n",
    "    args.validation_datasets = flatten_once(args.validation_datasets)\n",
    "  else:\n",
    "    args.validation_datasets = []\n",
    "\n",
    "  for dataset in args.datasets:\n",
    "    if not os.path.exists(dataset):\n",
    "      raise Exception('{0} does not exist'.format(dataset))\n",
    "\n",
    "  for val_dataset in args.validation_datasets:\n",
    "    if not os.path.exists(val_dataset):\n",
    "      raise Exception('{0} does not exist'.format(val_dataset))\n",
    "\n",
    "  train_datasets = args.datasets\n",
    "  val_datasets = train_datasets\n",
    "  if len(args.validation_datasets) > 0:\n",
    "    val_datasets = args.validation_datasets\n",
    "\n",
    "  if (args.start_lambda is not None) != (args.end_lambda is not None):\n",
    "    raise Exception('Either both or none of start_lambda and end_lambda must be specified.')\n",
    "\n",
    "  feature_set = features.get_feature_set_from_name(args.features)\n",
    "\n",
    "  start_lambda = args.start_lambda or args.lambda_\n",
    "  end_lambda = args.end_lambda or args.lambda_\n",
    "  max_epoch = args.max_epochs or 800\n",
    "  \n",
    "  nnue = torch.load(args.resume_from_model)\n",
    "  nnue.set_feature_set(feature_set)\n",
    "  nnue.start_lambda = start_lambda\n",
    "  nnue.end_lambda = end_lambda\n",
    "  nnue.max_epoch = max_epoch\n",
    "  # we can set the following here just like that because when resuming\n",
    "  # from .pt the optimizer is only created after the training is started\n",
    "  nnue.gamma = args.gamma\n",
    "  nnue.lr = args.lr\n",
    "  nnue.param_index=args.param_index\n",
    "\n",
    "  print(\"Feature set: {}\".format(feature_set.name))\n",
    "  print(\"Num real features: {}\".format(feature_set.num_real_features))\n",
    "  print(\"Num virtual features: {}\".format(feature_set.num_virtual_features))\n",
    "  print(\"Num features: {}\".format(feature_set.num_features))\n",
    "\n",
    "  print(\"Training with: {}\".format(train_datasets))\n",
    "  print(\"Validating with: {}\".format(val_datasets))\n",
    "\n",
    "  pl.seed_everything(args.seed)\n",
    "  print(\"Seed {}\".format(args.seed))\n",
    "\n",
    "  batch_size = args.batch_size\n",
    "  if batch_size <= 0:\n",
    "    batch_size = 16384\n",
    "  print('Using batch size {}'.format(batch_size))\n",
    "\n",
    "  print('Smart fen skipping: {}'.format(not args.no_smart_fen_skipping))\n",
    "  print('WLD fen skipping: {}'.format(not args.no_wld_fen_skipping))\n",
    "  print('Random fen skipping: {}'.format(args.random_fen_skipping))\n",
    "  print('Skip early plies: {}'.format(args.early_fen_skipping))\n",
    "  print('Param index: {}'.format(args.param_index))\n",
    "\n",
    "  if args.threads > 0:\n",
    "    print('limiting torch to {} threads.'.format(args.threads))\n",
    "    t_set_num_threads(args.threads)\n",
    "\n",
    "  logdir = args.default_root_dir if args.default_root_dir else 'logs/'\n",
    "  print('Using log dir {}'.format(logdir), flush=True)\n",
    "\n",
    "  tb_logger = pl_loggers.TensorBoardLogger(logdir)\n",
    "  checkpoint_callback = pl.callbacks.ModelCheckpoint(save_last=args.save_last_network, every_n_epochs=args.network_save_period, save_top_k=-1)\n",
    "  trainer = pl.Trainer.from_argparse_args(args, callbacks=[checkpoint_callback], logger=tb_logger)\n",
    "\n",
    "  main_device = trainer.strategy.root_device if trainer.strategy.root_device.index is None else 'cuda:' + str(trainer.strategy.root_device.index)\n",
    "\n",
    "  nnue.to(device=main_device)\n",
    "\n",
    "  print('Using c++ data loader')\n",
    "  train, val = make_data_loaders(\n",
    "    train_datasets,\n",
    "    val_datasets,\n",
    "    feature_set,\n",
    "    args.num_workers,\n",
    "    batch_size,\n",
    "    not args.no_smart_fen_skipping,\n",
    "    args.random_fen_skipping,\n",
    "    not args.no_wld_fen_skipping,\n",
    "    args.early_fen_skipping,\n",
    "    args.param_index,\n",
    "    main_device,\n",
    "    args.epoch_size,\n",
    "    args.validation_size)\n",
    "  \n",
    "  batch1 = train[0]\n",
    "\n",
    "  us, them, white_indices, white_values, black_indices, black_values, outcome, score, psqt_indices, layer_stack_indices = batch1\n",
    "\n",
    "  scorenet = nnue(us, them, white_indices, white_values, black_indices, black_values, psqt_indices, layer_stack_indices)\n",
    "\n",
    "  mean = 0.022645641423111207\n",
    "  std = 0.052114194767384596  \n",
    "\n",
    "  p_norm = (score - mean) / std\n",
    "\n",
    "  print(torch.pow(torch.abs(p_norm - scorenet), 2).mean())\n",
    "\n",
    "  losses = []\n",
    "  \n",
    "  # for batch in train:\n",
    "  #   us, them, white_indices, white_values, black_indices, black_values, outcome, score, psqt_indices, layer_stack_indices = batch\n",
    "\n",
    "  #   mean = 0.022645641423111207\n",
    "  #   std = 0.052114194767384596\n",
    "\n",
    "  #   p_norm = (score - mean) / std\n",
    "\n",
    "  #   scorenet = nnue(us, them, white_indices, white_values, black_indices, black_values, psqt_indices, layer_stack_indices)\n",
    "    \n",
    "  #   loss = torch.pow(torch.abs(p_norm - scorenet), 2).mean()\n",
    "\n",
    "  #   losses.append(loss)\n",
    "\n",
    "\n",
    "  with open(os.path.join(logdir, 'training_finished'), 'w'):\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n",
    "  if sys.platform == \"win32\":\n",
    "    os.system(f'wmic process where processid=\"{os.getpid()}\" call terminate >nul')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d97e3-97bd-4cd8-a3c9-0d42fc23251c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da906c-6942-4e74-b849-896860e01cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
